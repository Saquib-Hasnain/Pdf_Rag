# ğŸ§  RAG PDF Chatbot using LangChain, OpenAI & Qdrant

This project is a working example of **Retrieval-Augmented Generation (RAG)** that lets you chat with any PDF ğŸ“„ using OpenAI's LLMs.

Instead of fine-tuning a model, we **store your PDF data in a vector database** and retrieve relevant chunks on-the-fly when a user asks a question.

---

## ğŸ“Œ Features

âœ… Chunk and index any PDF  
âœ… Store vector embeddings in **Qdrant**  
âœ… Ask natural language questions  
âœ… Get contextual answers with page references  
âœ… All in one Python script (`main.py`)

---

## âš™ï¸ How It Works

### Step-by-Step:

1. **Extract PDF content** and split into chunks  
2. **Generate embeddings** using `OpenAIEmbeddings`  
3. **Store** the vectors in `Qdrant` DB  
4. **User enters a query**
5. Query is embedded â†’ relevant chunks fetched via semantic search  
6. LLM is prompted with those chunks as context  
7. Answer is generated ğŸ”¥

---

## ğŸ“‚ Project Structure

â”œâ”€â”€ main.py # All-in-one PDF index + chat script

â”œâ”€â”€ nodejs.pdf # Your PDF file (rename or replace as needed)

â”œâ”€â”€ .env # API key and environment configs

â”œâ”€â”€ requirements.txt # All required Python dependencies

â””â”€â”€ docker-compose.yml # Qdrant setup




---

## ğŸš€ Getting Started

###  Clone the Repo

```bash
git clone https://github.com/yourusername/rag-pdf-chatbot.git
cd rag-pdf-chatbot


